# 27 | 决策树：信息增益、增益比率和基尼指数的运用


## 如何通过信息熵挑选合适的问题？

每次选择问题的时候，我们可以选择信息增益最高的问题，这样熵值下降得就最快。

第一步，根据分组中的人物类型，为每个集合计算信息熵，并通过全部集合的熵值加权平均，获得整个数据集的熵。注意，一开始集合只有一个，并且包含了所有的武侠人物。

第二步，根据信息增益，计算每个问卷题的区分能力。挑选区分能力最强的题目，并对每个集合进行更细的划分。

第三步，有了新的划分之后，回到第一步，重复第一和第二步，直到没有更多特征在进行划分。

这个过程就体现了训练决策树（Decision Tree）的基本思想。
每个人的熟悉并不完全一致，但分类都是相同，这样子决策树只能把整体的熵值降到一个最低的值，无法是0，这也意味着，训练得到的决策树模型，常常无法完全准确地划分训练样本，只能求到一个近似的解。

## 集中决策树算法

- ID3（Iterative Dichotomiser 3，迭代二叉树 3 代）

一般会优先考虑具有较多取值的特征

- C4.5 算法

在ID3算法的基础上进行改进



ID3算法缺点：

>更多的取值会把数据样本划分为更多更小的分组，这样熵就会大幅>降低，信息增益就会大幅上升。但是这样构建出来的树，很容易导>致机器学习中的过拟合现象，不利于决策树对新数据的预测。




使用信息增益率（Information Gain Ratio）来替代信息增益，作为选择特征的标准，并降低决策树过拟合的程度。信息增益率通过引入一个被称作分裂信息（Split Information）的项来惩罚取值较多的特征。





- CART 算法



在分类时，CART 不再采用信息增益或信息增益率，而是采用基尼指数（Gini）来选择最好的特征并进行数据的划分；在 ID3 和 C4.5 决策树中，算法根据特征的属性值划分数据，可能会划分出多个组。

而 CART 算法采用了二叉树，每次把数据切成两份，分别进入左子树、右子树。
