
# 33 | 线性代数：线性代数到底都讲了些什么？

## 前言

概率统计关注的是随机变量及其概率分布，以及如何通过观测数据来推断这些分布。

在信息检索中，我们需要考虑多个关键词特征对最终相关性的影响，而在机器学习中，无论是监督式还是非监督式学习，我们都需要考虑多个特征对模型拟合的影响

在研究多个变量之间关系的时候，线性代数成为了解决这类问题的有力工具


## 向量和向量空间

### 标量（Scalar

只是一个单独的数字，而且不能表示方向。
标量就是编程中最基本的变量

### 向量（Vector）

标量对应的概念，就是线性代数中最常用、也最重要的概念,代表一组数字，并且这些数字是有序排列的.

如：x1​，x2​，x3​，…，xn​ 等等，来表示向量中的每个元素，这里面的 n 就是向量的维

### 区别

向量和标量最大的区别在于，向量除了拥有数值的大小，还拥有方向。向量或者矢量中的“向”和“矢”这两个字，都表明它们是有方向的。

### 特征向量（Feature Vector）

可以使用向量来表示某个物体的特征。其中，向量的每个元素就代表一维特征，而元素的值代表了相应特征的值

### 矩阵的特征向量（Eigenvector

矩阵的几何意义是坐标的变换。如果一个矩阵存在特征向量和特征值，那么这个矩阵的特征向量就表示了它在空间中最主要的运动方向。


## 向量的运算

先定义向量空间

空间主要有几个特性：

空间由无穷多个的位置点组成；这些点之间存在相对的关系；可以在空间中定义任意两点之间的长度，以及任意两个向量之间的角度；这个空间的点可以进行移动。


两个向量之间的加法，首先它们需要维度相同，然后是对应的元素相加。


向量之间的乘法默认是点乘，向量 x 和 y 的点乘是这么定义的：
点乘的作用是把相乘的两个向量转换成了标量


### 矩阵的运算

 矩阵由多个长度相等的向量组成，其中的每列或者每行就是一个向量。因此，我们把向量延伸一下就能得到矩阵（Matrix）。

 矩阵 X 的行向量和矩阵 Y 的列向量两两进行点乘，称它为元素对应乘积，或者 Hadamard 乘积。


 转置（Transposition）是指矩阵内的元素行索引和纵索引互换，例如 Xij​ 就变为 Xji​，


 逆矩阵

 如果有矩阵 X，我们把它的逆矩阵记做 X−1，两者相乘的结果是单位矩阵
 
