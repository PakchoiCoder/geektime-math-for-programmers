
# 28 | 熵、信息增益和卡方：如何寻找关键特征？

## 什么是特征选择？

它主要包括监督式学习（Supervised Learning）和非监督式的学习（Unsupervised Learning）。

监督式学习，是指通过训练资料学习并建立一个模型，并依此模型推测新的实例，主要包括分类（Classification）和回归（Regression）。

无论是在监督学习还是非监督学习中，我们都可以使用特征选择。


### 监督式机器学习的主要步骤

机器学习的步骤主要包括数据的准备、特征工程、模型拟合、离线和在线测试。

特征”（Feature），是机器学习非常常用的术语，它其实就是可用于模型拟合的各种数据。



例如：

现实世界中水果的各类特征转化为计算机所能理解的数据，这个过程其实就是最初级的特征工程


特征工程还包括特征选择、缺失值的填补和异常值的去除等等。


 针对数据类型和维度的出现，这种情形。特征选择尝试发掘和预定义任务相关的特征，同时过滤不必要的噪音特征。

 可以使用穷举法来找到最优的结果，但是如果特征有 N 个，那么复杂度会达到 O(2N)，但是穷举法并不适合特征数量庞大的问题。


 ### 利用信息熵进行特征选择

一个好的特征选择，应该可以把那些对分类有价值的信息提取出来，而过滤掉那些对分类没有什么价值的信息。

#### 什么是对分类有价值的特征？

如果一个特征，经常只在某个或少数几个分类中出现，而很少在其他分类中出现，那么说明这个特征具有较强的区分力，它的出现很可能预示着整个数据属于某个分类的概率很高或很低。

##### 如何判断特征是少数的。


- 单个特征

是否属于少数几个类这一点，可以使用信息熵来衡量。我用 Dfi​ 来表示所有出现特征 fi​ 的数据集合，这个集合一共包含了 n 个分类 C，而 cj​ 表示这 n 个分类中的第 j 个。然后我们就可以根据 Dfi​ 中分类 C 的分布，来计算熵。

- 多个特征

可以借用决策树中信息增益的概念。我们把单个特征 f 是不是出现作为一个决策条件，将数据集分为 Dfi​ 和 Dfi​ˉ​ ，Dfi​ 表示出现了这个特征的数据，而 Dfi​ˉ​ 表示没有出现这个特征的数据。那么使用特征 fi​ 进行数据划分之后，我们就能得到基于两个新数据集的熵，然后和没有划分之前的熵进行比较，得出信息增益。

如果基于某个特征的划分，所产生的信息增益越大，说明这个特征对于分类的判断越有价值。

### 利用卡方检验进行特征选择

在统计学中，我们使用卡方检验来检验两个变量是否相互独立。
可以检验特征与分类这两个变量是否独立。如果两者独立，证明特征和分类没有明显的相关性，特征对于分类来说没有提供足够的信息量。反之，如果两者有较强的相关性，那么特征对于分类来说就是有信息量的，是个好的特征。

独立性

卡方检验考虑了四种情况的概率：P(fi​,cj​) 、P(fi​ˉ​,cj​ˉ​)、P(fi​,cj​ˉ​) 和 P(fi​ˉ​,cj​)。

#### 正相关

P(fi​,cj​) 和 P(fi​ˉ​,cj​ˉ​) 表示特征 fi​ 和分类 cj​ 是正相关的。

- 如果 P(fi​,cj​) 很高，表示特征 fi 的出现意味着属于分类 cj​ 的概率更高；

- 如果 P(fi​ˉ​,cj​ˉ​) 很高，表示特征 fi​ 不出现意味着不属于分类 cj​ 的概率更高

#### 负相关

 P(fi​,cj​ˉ​) 和 P(fi​ˉ​,cj​) 表示特征 fi​ 和分类 cj​ 是负相关的。

- 如果 P(fi​,cj​ˉ​) 很高，表示特征 fi​ 的出现意味着不属于分类 cj​ 的概率更高；

- 如果 P(fi​ˉ​,cj​) 很高，表示特征 fi​ 不出现意味着属于分类 cj​ 的概率更高。


如果特征和分类的相关性很高，要么是正向相关值远远大于负向相关值，要么是负向相关值远远大于正向相关值。

如果特征和分类相关性很低，那么正向相关值和负向相关的值就会很接近。



如果一个特征和分类的相关性很高，无论是正向相关还是负向相关，那么正向相关和负向相关的差值就很大，最终计算的值就很高。

可以按照卡方检验的值由高到低对特征进行排序，挑选出排列靠前的特征。