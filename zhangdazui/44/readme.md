# 44 | 奇异值分解：如何挖掘潜在的语义关系？

SVD 奇异值分解（Singular Value Decomposition）。它的核心思路和 PCA 不同。PCA 是通过分析不同纬度特征之间的协方差，找到包含最多信息量的特征向量，从而实现降维。而 SVD 这种方法试图通过样本矩阵本身的分解，找到一些“潜在的因素”，然后通过把原始的特征维度映射到较少的潜在因素之上，达到降维的目的。

## 方阵的特征分解

**方阵（**Square Matrix）是一种特殊的矩阵，它的行数和列数相等。如果一个矩阵的行数和列数都是 n，那么我们把它称作 n 阶方阵。

如果一个矩阵和其转置矩阵相乘得到的是单位矩阵，那么它就是一个**酉矩阵**（Unitary Matrix）。

## 矩阵的奇异值分解

VD 并不要求要分解的矩阵为方阵，所以这里的 U 和 V’ 并不是互为逆矩阵。其中 U 是一个 m×m 维的矩阵，V 是一个 n×n 维的矩阵。而 Σ 是一个 m×n 维的矩阵，对于 Σ 来说，只有主对角线之上的元素可以为非 0，其他元素都是 0，而主对角线上的每个元素就称为奇异值。U 和 V 都是酉矩阵，即满足 U’U=I,V’V=I。

我们就得到了矩阵 X’X 的 n 个特征值和对应的 n 个特征向量 v。通过 X’X 的所有特征向量构造一个 n×n 维的矩阵 V，这就是上述 SVD 公式里面的 V 矩阵了。通常我们把 V 中的每个特征向量叫作 X 的**右奇异向量**。

我们得到了矩阵 XX’ 的 m 个特征值和对应的 m 个特征向量 u。通过 XX’的所有特征向量构造一个 m×m 的矩阵 U。这就是上述 SVD 公式里面的 U 矩阵了。通常，我们把 U 中的每个特征向量叫作 X 的**左奇异向量**。

## 潜在语义分析和 SVD

LSA 主要包括以下这些步骤。

第一步，分析文档集合，建立表示文档和词条关系的矩阵。

第二步，对文档 - 词条矩阵进行 SVD 奇异值分解。在 LSA 的应用场景下，分解之后所得到的奇异值σ对应了一个语义上的“概念”，而 σ 值的大小表示这个概念在整个文档集合中的重要程度。U 中的左奇异值向量表示了每个文档和这些语义“概念”的关系强弱，V 中的右奇异值向量表示每个词条和这些语义“概念”的关系强弱。所以说，SVD 分解把原来的词条 - 文档关系，转换成了词条 - 语义概念 - 文档关系。

第三步，对 SVD 分解后的矩阵进行降维，这个操作和 PCA 主成分分析的降维操作是类似的。

第四步，使用降维后的矩阵重新构建概念 - 文档矩阵，新矩阵中的元素不再表示词条是不是出现在文档中，而是表示某个概念是不是出现在文档中。