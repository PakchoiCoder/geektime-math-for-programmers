# 28 | 熵、信息增益和卡方：如何寻找关键特征？

选取更有价值的特征，以提升模型的效率。

## 什么是特征选择？

**监督式学习**：是指通过训练资料学习并建立一个模型，并依此模型推测新的实例，主要包括分类（Classification）和回归（Regression）。

**非监督式学习**：是指在没有类别信息情况下，通过对所研究对象的大量样本的数据分析实现对样本分类的一种数据处理方法。算法本身将发掘数据中有趣的结构。

**机器学习的步骤**：主要包括数据的准备、特征工程、模型拟合、离线和在线测试。

**“特征”（Feature）**：是机器学习非常常用的术语，它其实就是可用于模型拟合的各种数据。

越来越多的数据类型和维度的出现，会加大机器学习的难度，并影响最终的准确度。不选择特征会有 O(2N) 这种数量级的特征子集搜索针。对这种情形，特征选择尝试发掘和预定义任务相关的特征，同时过滤不必要的噪音特征。它主要包括特征子集的产生、搜索和评估。

## 利用信息熵进行特征选择

如果一个特征，经常只在某个或少数几个分类中出现，而很少在其他分类中出现，那么说明这个特征具有较强的区分力，它的出现很可能预示着整个数据属于某个分类的概率很高或很低。

是否属于少数几个类这一点，可以使用信息熵来衡量。如果熵值很低，说明包含这个特征的数据只出现在少数分类中，对于分类的判断有价值。计算出每个特征所对应的数据集之熵，我们就可以按照熵值由低到高对特征进行排序，挑选出排列靠前的特征。

我们可以为计算基于每个特征的划分，所产生的信息增益，然后按照增益值由高到低对特征进行排序，挑选出排列靠前的特征。

## 利用卡方检验进行特征选择

卡方检验来检验两个变量是否相互独立。

把它运用到特征选择，我们就可以检验特征与分类这两个变量是否独立。如果两者独立，证明特征和分类没有明显的相关性，特征对于分类来说没有提供足够的信息量。反之，如果两者有较强的相关性，那么特征对于分类来说就是有信息量的，是个好的特征。
