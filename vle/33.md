#### 33 | 线性代数：线性代数到底都讲了些什么？
1. 在研究多个变量之间关系的时候，线性代数成为了解决这类问题的有力工具
2. 向量和向量空间
    1. 标量（Scalar），它只是一个单独的数字，而且不能表示方向
    2. 向量（Vector），也叫做矢量，它代表一组数字，并且这些数字时有序排列的
    3. 向量和标量最大的区别在于，向量除了拥有数值的大小，还拥有方向
        1. 为什么这么一串数字能表示方向？
        2. 这是因为，如果我们把向量中的元素看成坐标轴上的坐标，那么这个向量就可以看作空间中的一个点，以原点为起点，以向量代表的点为重点，就能形成一条有向的直线。
        3. 以上处理就给向量赋予了代数的含义，使得计算的过程中更加直观。
            1. 向量空间
            2. 向量夹角
            3. 矩阵特征值
    4. 自然界物体的属性转换为用数字表达，向量的每个元素代表一维特征，而元素的值代表相应特征的值，我们称这类向量为特征向量（Feature Vector）
        1. 这个和矩阵的特征向量（Eigenvector）是两码事
3. 向量的运算
    1. 标量和向量之间可以进行运算，把标量和向量中的每个元素进行运算
    2. 向量和向量之间运算需要先定义向量空间
    3. 向量空间
        1. 空间由无穷多个的位置点组成
        2. 这些点之间存在相对的关系
        3. 可以在空间中定义任意两点之间的长度，以及任意两点之间的角度
        4. 这个空间的点可以进行移动
        5. 定义运算
            1. 加法：首先两个向量需要维度相同，然后是对应的元素相加
            2. 乘法（点乘）：向量之间的乘法默认是点乘，点乘的作用是把相乘的两个向量转换成了标量。它有具体的几何含义。我们会用点乘来计算向量的长度以及两个向量的夹角。向量之间的夹角和距离又在向量空间模型（Vector Space Model）中发挥了重要作用。信息检索和机器学习等领域利用了向量空间模型来计算不同对象之间的相似程度
            1. 距离
            2. 夹角
4. 矩阵（Matrix）
    1. 矩阵由多个长度相等的向量组成
        1. 向量其实也是一个特殊的矩阵
        2. 从数据结构角度看，向量是一维数组，那矩阵就是二维数组
        3. 如果二维数组里面大多数元素都是0，我们称这个矩阵很稀疏（Sparse）
            1. 我们可以使用哈希表的链地址法表示稀疏矩阵
        4. 单位矩阵（identity Matrix）：所有沿主对角线的元素都是1，其他位置的元素都是0，通常我们只考虑单位矩阵为方阵的情况（即行数和列数相等）
    2. 矩阵的几何意义是坐标变换
        1. 特征向量
        2. 特征值
        3. 如果一个矩阵存在特征向量和特征值，那么这个矩阵的特征向量就表示了它在空间中最主要的运动方向
    3. 矩阵的运算
        1. 标量和矩阵之间的运算，把标量和矩阵中的每个元素进行运算
        2. 矩阵和矩阵之间
            1. 加法：首先保证参与运算的两个矩阵具有相同的行维度和列维度，我们就可以把对应的元素两两相加
            2. 乘法：假设X为i*k的矩阵，Y为k*j的矩阵，首先X的列数必须要和Y的行数相等，计算X的行向量与Y的列向量的点乘
            3. 转置（transposition）：将矩阵内的元素行索引和纵索隐呼唤，转置N*M的矩阵得到M*N的矩阵
            4. 求逆矩阵：逆矩阵*原始矩阵，结果是单位矩阵
            5. 求特征值
            6. 求奇异值
