#### 23 | 文本分类：如何区分特定类型的新闻？

#### 文本分类系统的基本框架
> 1. 采集训练样本
> 2. 预处理自然语言
> 3. 训练模型
> 4. 实时分类预测

#### 基于自然语言的预处理

1. 分词计算机处理自然语言的基本单位是单词和词组。对于英语等拉丁语系的语言来说，单词之间是以空格作为自然分界符的，所以我们可以直接使用空格对句子进行分割，然后来获取每个单词。
但是，中文、日文、韩文这些语言在书写的时候，词和词之间并没有空格可以进行自然分界，所以我们就需要使用一些算法，来估计词语之间的划分，我们将这个过程称为分词。

* 第一种是基于字符串匹配
* 第二种是基于统计和机器学习。
这类分词基于人工标注的词性和统计特征，对中文进行建模。
训练阶段，根据标注好的语料对模型参数进行估计。 
在分词阶段再通过模型计算各种分词出现的概率，将概率最大的分词作为最终结果。
常见的序列标注模型有隐马尔科夫模型（HMM，Hidden Markov Model）和条件随机场（CRF，Conditional Random Field）
> 2. 取词干和归一化
> 3. 停用词无论何种语言，都会存在一些不影响（或基本不影响）相关性的词。
>有的时候干脆可以指定一个叫停用词（stop word）的字典，直接将这些词过滤，不予以考虑。例如英文中的 a、an、the、that、is、good、bad 等。中文“的、个、你、我、他、好、坏”等。如此一来，我们可以在基本不损失语义的情况下，减少数据文件的大小，提高计算机处理的效率。
>当然，也要注意停用词的使用场景，例如用户观点分析，good 和 bad 这样的形容词反而成为了关键。不仅不能过滤，反而要加大它们的权重

> 4. 同义词和扩展词
>
>自然语言的处理是关键的预处理步骤，它将文本转换成计算机所能处理的数据。
>常见方法包括中文分词，英文的取词干和归一化，还有适用于各种语言的停用词、同义词和扩展词等。如果不考虑这些词出现的先后顺序，以及表达的深层次语义，那么我们就可以使用词包的方法，将大段的文章和语句转化成单词所组成的集合。
>之后，我们就能统计每个单词属于每个分类的条件概率，以及分类和单词的先验概率

