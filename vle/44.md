44 | 奇异值分解：如何挖掘潜在的语义关系

#### SVD 奇异值分解（Singular Value Decomposition）
它的核心思路和 PCA 不同。PCA 是通过分析不同纬度特征之间的协方差，找到包含最多信息量的特征向量，从而实现降维。
而 SVD 这种方法试图通过样本矩阵本身的分解，找到一些“潜在的因素”，然后通过把原始的特征维度映射到较少的潜在因素之上，达到降维的目的

#### 方阵的特征分解
>> 方阵（Square Matrix）是一种特殊的矩阵，它的行数和列数相等。如果一个矩阵的行数和列数都是 n，那么我们把它称作 n 阶方阵
>> 如果一个矩阵和其转置矩阵相乘得到的是单位矩阵，那么它就是一个酉矩阵（Unitary Matrix)

X’X=I其中 X’表示 X 的转置，I 表示单位矩阵。换句话说，矩阵 X 为酉矩阵的充分必要条件是 X 的转置矩阵和 X 的逆矩阵相等。X’=X−1

### 潜在语义分析和 SVD
1. 第一步，分析文档集合，建立表示文档和词条关系的矩阵。
2. 第二步，对文档 - 词条矩阵进行 SVD 奇异值分解。在 LSA 的应用场景下，分解之后所得到的奇异值σ对应了一个语义上的“概念”，而 σ 值的大小表示这个概念在整个文档集合中的重要程度。U 中的左奇异值向量表示了每个文档和这些语义“概念”的关系强弱，V 中的右奇异值向量表示每个词条和这些语义“概念”的关系强弱。所以说，SVD 分解把原来的词条 - 文档关系，转换成了词条 - 语义概念 - 文档关系
3. 第三步，对 SVD 分解后的矩阵进行降维，这个操作和 PCA 主成分分析的降维操作是类似的
4. 第四步，使用降维后的矩阵重新构建概念 - 文档矩阵，新矩阵中的元素不再表示词条是不是出现在文档中，而是表示某个概念是不是出现在文档中。

> 总的来说，LSA 的分解，不仅可以帮助我们找到词条之间的语义关系，还可以降低向量空间的维度。在这个基础之上在运行其他的信息检索或者机器学习算法，就更加有效
