42 | PCA主成分分析（上）：如何利用协方差矩阵来降维？
在概率统计模块，我详细讲解了如何使用各种统计指标来进行特征的选择，降低用于监督式学习的特征之维度。接下来的几节，我会阐述两种针对数值型特征，更为通用的降维方法，它们是主成分分析 PCA（Principal Component Analysis）和奇异值分解 SVD（Singular Value Decomposition）。这两种方法是从矩阵分析的角度出发，找出数据分布之间的关系，从而达到降低维度的目的，因此并不需要监督式学习中样本标签和特征之间的关系
#### PCA 分析法的主要步骤

这个矩阵是 m×n 维的，其中每一行表示一个样本，而每一列表示一维特征。让我们把这个矩阵称作样本矩阵，现在，我们的问题是，能不能通过某种方法，找到一种变换，可以降低这个矩阵的列数，也就是特征的维数，并且尽可能的保留原始数据中有用的信息？针对这个问题，PCA 分析法提出了一种可行的解决方案。它包括了下面这样几个主要的步骤d：标准化样本矩阵中的原始数据；获取标准化数据的协方差矩阵；计算协方差矩阵的特征值和特征向量；依照特征值的大小，挑选主要的特征向量；生成新的特征。

1. 标准化原始数据
之前我们已经介绍过特征标准化，这里我们需要进行同样的处理，才能让每维特征的重要性具有可比性。为了便于你回顾，我把标准化的公式列在了这里。x’=σx−μ​其中 x 为原始值，u 为均值，σ 为标准差，x’ 是变换后的值。需要注意的是，这里标准化的数据是针对同一种特征，也是在同一个特征维度之内。不同维度的特征不能放在一起进行标准化
2. 获取协方差矩阵首先，我们来看一下什么是协方差（Covariance），以及协方差矩阵。协方差是用于衡量两个变量的总体误差。假设两个变量分别是 x 和 y，而它们的采样数量都是 m，那么协方差的计算公式就是如下这种形式

3. 计算协方差矩阵的特征值和特征向量
需要注意的是，这里所说的矩阵的特征向量，和机器学习中的特征向量（Feature Vector）完全是两回事。矩阵的特征值和特征向量是线性代数中两个非常重要的概念。对于一个矩阵 X，如果能找到向量 v 和标量 λ，使得下面这个式子成立

4. 挑选主要的特征向量，转换原始数据
假设我们获得了 k 个特征值和对应的特征向量，那么我们就有：Xv1​=λ1​v1​Xv2​=λ2​v2​…Xvk​=λk​vk​按照所对应的λ数值的大小，对这 k 组的 v 排序。排名靠前的 v 就是最重要的特征向量。假设我们只取前 k1 个最重要的特征，那么我们使用这 k1 个特征向量，组成一个 n×k1 维的矩阵 D。把包含原始数据的 m×n 维矩阵 X 左乘矩阵 D，就能重新获得一个 m×k1 维的矩阵，达到了降维的目的。

