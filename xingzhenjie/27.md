# 27 | 决策树：信息增益、增益比率和基尼指数的运用

## 如何通过信息熵挑选合适的问题？

**决策树学习属于归纳推理算法之一，适用于分类问题。**

分类算法只要包括建立模型和分类新数据两个阶段。决定问卷题目出现顺序的这个过程，其实就是建立决策树模型的过程。

**决策树构建树的步骤：**

* 第一步，根据集合中的样本分类，为每个集合计算信息熵，并通过全部集合的熵之加权平均，获得整个数据集的熵。注意，**一开始集合只有一个，并且包含了所有的样本。**
* 第二步，根据信息增益，计算每个特征的区分能力。挑选区分能力最强的特征，并对每个集合进行更细的划分。
* 第三步，有了新的划分之后，回到第一步，重复第一步和第二步，直到没有更多的特征，或者所有的样本都已经被分好类。

## 几种决策树算法的异同

采用信息增益来构建决策树的算法成为ID3（迭代二叉树3代）。缺点：一般会优先考虑具有较多取值的特征，因为取之多的特征会有相对较大的信息增益。

更多的取值会把数据样本划分为更多更小的分组，这样熵就会大幅降低，信息增益就会大幅上升。但是这样构建出来的树，很容易导致机器学习中的过拟合现象，不利于决策树对新数据的预测。

改进算法C4.5使用**信息增益率**来替代信息增益，作为选择特征的标准，并降低决策树过拟合的程度。信息增益率通过引入一个被称为**分裂信息**的项来惩罚取值较多的特征。

另一种常见决策树是CART算法（分类与回归树）

与ID3、C4.5主要有2处不同：

* 在分类时，CART不再采用信息增益或信息增益率，而是采用基尼指数来选择最好的特征并进行数据的划分；
* 在ID3和C4.5决策树中，算法根据特征的属性值划分数据，可能会划分出多个组。而CART算法采用了二叉树，每次把数据切成两份，分别进入左子树、右子树。

CART中每一次迭代都会降低基尼指数，类似于ID3、C4.5降低信息熵的过程。基尼指数描述的也是纯度，与信息熵的含义相似。

## 总结

决策树算法的优势在于：容易理解和实现。此外，对于通过样本训练所得的树结构，其每个结点都是基于某个数据特征的判定，对于我们的阅读和解释来说都是很方便的。

决策树的不足：这类算法受训练样本的影响很大，比较容易过拟合。在测试阶段，如果新的数据和原来的训练样本差异较大，那么分类效果就会比较差。为此提出优化方案，比如剪枝和随机森林。


********





