# 44 | 奇异值分解：如何挖掘潜在的语义关系？

**SVD奇异值分解：**试图通过样本矩阵本身的分解，找到一些“潜在的因素”，然后通过把原始的特征维度映射到较少的潜在因素之上，达到降维的目的。

## 方阵的特征分解

**方阵**是一种特殊的矩阵，它的行数和列数相等。如果一个矩阵的行数和列数都是n，那么称之为n阶方阵。

如果一个矩阵和其转置矩阵相乘得到的是单位矩阵，那么它就是一个**酉矩阵**。

**特征分解**，是指把矩阵分解为其特征值和特征向量表示的矩阵之积的方法。

## 矩阵的奇异值分解

**奇异值**

**右奇异向量**

**左奇异向量**

## 潜在语义分析和SVD

### 潜在语义分析LSA

和一般的向量空间模型有所不同，LSA通过词条和文档所组成的矩阵，发掘词和词之间的语义关系，并过滤掉原始向量空间中存在的一些“噪音”，最终提高信息检索和机器学习算法的精确度。LSA主要包括以下步骤：

* 第一步，分析文档集合建立表示文档和词条关系的矩阵。
* 第二步，对文档-词条矩阵进行SVD奇异值分解。在LSA的应用场景下，分解至后所得到的奇异值对应了一个语义上的“概念”，而值的大小表示这个概念在整个翁当集合中的重要程度。U中的左奇异值向量表示了每个文档和这些语义“概念”的关系强弱，V中的右奇异值向量表示每个词条和这些语义“概念”的关系强弱。所以说，SVD分解把原来的词条-文档关系，转化成了词条-语义概念-文档关系。
* 第三步，对SVD分解后的矩阵进行降维，这个操作和PCA主成分分析的降维操作是类似的。
* 第四步，使用降维后的矩阵重新构建概念-文档矩阵，新矩阵中的元素不再表示词条是不是出现在文档中，而是表示某个概念是不是出现在文档中。

********









