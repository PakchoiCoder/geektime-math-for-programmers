# PCA主成分分析（上）：如何利用协方差矩阵来降维？
- 降低用于监督式学习的特征之维度,分为主成分分析 PCA（Principal Component Analysis）和奇异值分解 SVD（Singular Value Decomposition）。这两种方法是从矩阵分析的角度出发，找出数据分布之间的关系，从而达到降低维度的目的
### PCA
- why为什么要降维
> 在机器学习领域中，我们要进行大量的特征工程，把物品的特征转换成计算机所能处理的各种数据。通常，我们增加物品的特征，就有可能提升机器学习的效果。可是，随着特征数量不断的增加，特征向量的维度也会不断上升。这不仅会加大机器学习的难度，还会影响最终的准确度。针对这种情形，我们需要过滤掉一些不重要的特征，或者是把某些相关的特征合并起来，最终达到在减少特征维度的同时，尽量保留原始数据所包含的信息。
- m*n的一个矩阵 m为m个样本，n为n哥特征向量
#### 具体步骤
- 标准化原始数据，在同一个特征维度内，其中 x 为原始值，u 为均值，σ 为标准差，x’ 是变换后的值， x' = x -u/σ
- 获取协方差矩阵,假设两个变量分别是 x 和 y，而它们的采样数量都是 m，那么协方差的计算公式就是如下这种形式：cov(x,y)=((Xk-u)(Yk-u)所有的和)/m-1
> 协方差矩阵就是 所有原始矩阵中所有变量预变量间的协方差组成的
- 计算协方差矩阵的特征值和特征向量需要注意的是，这里所说的矩阵的特征向量，和机器，需要注意的是，这里所说的矩阵的特征向量，和机器学习中的特征向量（Feature Vector）完全是两回事。矩阵的特征值和特征向量是线性代数中两个非常重要的概念。对于一个矩阵 X，如果能找到向量 v 和标量 λ，使得下面这个式子成立。Xv=λv,我们为什么要关心这两个概念呢？简单的来说，我们可以把向量 v 左乘一个矩阵 X 看做对 v 进行旋转或拉伸，而这种旋转和拉伸都是由于左乘矩阵 X 后，所产生的“运动”所导致的。特征向量 v 表示了矩阵 X 运动的方向，特征值 λ 表示了运动的幅度，这两者结合就能描述左乘矩阵 X 所带来的效果，因此被看作矩阵的“特征”。在 PCA 中的主成分，就是指特征向量，而对应的特征值的大小，就表示这个特征向量或者说主成分的重要程度。特征值越大，重要程度越高，我们要优先现在这个主成分，并利用这个主成分对原始数据进行变换。
- 挑选主要的特征向量，转换原始数据,按照所对应的λ数值的大小，对这 k 组的 v 排序。排名靠前的 v 就是最重要的特征向量。

