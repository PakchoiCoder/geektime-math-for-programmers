# 44 | 奇异值分解：如何挖掘潜在的语义关系？

## SVD 奇异值分解（Singular Value Decomposition）降维

PCA 是通过分析不同纬特征之间的协方差，找到包含最多信息量的特征向量，从而实现降维。而 SVD 这种方法试图通过样本矩阵本身的分解，找到一些“潜在的因素”，然后通过把原始的特征维度映射到较少的潜在因素智商，达到降维的目的。

## 方阵的特征分解

方阵（Square Matrix）是一种特殊的矩阵，它的行数和列数相等，如果一个矩阵的行数和列数都是 n，那么我们把它称作 n 阶方阵。

如果一个矩阵和其转置矩阵相乘得到的是单位矩阵，那么它就是一个酉矩阵（Unitary Matrix）。

特征向量表示了矩阵变化的方向，而特征值表示了变化的幅度，通过特征值和特征矩阵，可以把矩阵 X 进行特征分解（Eigendecomposition）。这里矩阵的特征分解，是指把矩阵分解为由其特征值和特征向量表示的矩阵之积的方法。

矩阵 X 必须为对称方阵才能进行有实数解的特征分解。那么如果 X 不是方阵，就需要用到奇异值分解 SVD 了。

## 矩阵的奇异值分解

SVD 分解和特征分解相比，在形式上是类似的。假设矩阵 X 是一个 m×n 维的矩阵，那么 X 的 SVD 为 X=UΣV’。不同的地方在于，SVD 并不要求要分解的矩阵为方阵，所以这里的 U 和 V’ 并不是互为逆矩阵。

TODO
