# 24 | 语言模型：如何使用链式法则和马尔科夫假设简化概率模型？

# 语言模型

重要概念：

- 链式法则：把联合概率转化为条件概率
- 马尔可夫假设：马尔可夫假设通过变量间的独立性来减少条件概率中的随机变量

两者结合可以大幅简化计算的复杂度。

## 链式法则

使用一系列条件概率和边缘概率，来推导联合概率。

> 回顾一下：联合概率就是既满足于 X 条件，又满足于 Y 条件的概率；条件概率是假设某个变量固定，在这个固定条件下的概率；边缘概率是一件事情与其他事件无关的发生的概率，所以计算时可以排除其他概率的干扰。

## 马尔可夫假设

任何一个词 wi​ 出现的概率只和它前面的 1 个或若干个词有关。基于这个假设，我们可以提出多元文法（Ngram）模型。Ngram 中的“N”很重要，它表示任何一个词出现的概率，只和它前面的 N-1 个词有关。

以二元文法模型为例，表示某个单词出现的概率只和它前面的 1 个单词有关。

那么有什么用呢？这里举一个实际场景的例子：

假设有一篇文章和一个句子，我们想要统计这个文本中这个句子出现的可能性，那么就可以统计句子中所有单词在这个文本中出现的联合概率，就可以用链式法则进行推导，拆分成一系列的条件概率和边缘概率。

但是到了后面的单词，出现的概率会越来越小，趋向于 0 而且计算量会非常大。这时候 马尔可夫假设 和 多元文法模型 就可以帮忙了，比如三元文法模型，就可以让计算量减小，只需要计算当前单词跟前两个单词的情况下出现的概率。

## 语言模型的应用

主要应用在机器翻译、语音识别和中文分词。

### 信息检索

信息检索的核心问题就是想关心，给定一个查询，计算哪篇文章是相关的。

语言模型通过概率计算相关度。我们计算查询语句在文章中出现的可能概率，所以就是求 P(文章|查询语句) 即在给定查询语句的条件下求条件概率。

通过贝叶斯定理和部分忽略可以得到我们实际上求 P(查询语句｜文章) 的概率，然后根据链式法则和马尔可夫假设和多元文法进行继续推导，并得出概率。

将概率进行排序最终得出相关度。

### 中文分词

普通的分词方法是字典，按照字典进行匹配然后拆分，但是会有很多歧义的产生，而且字典并不一定十分完善齐全。

所以我们根据大量语料进行统计，然后来估算在这种场景下，比较常用的组合是是什么？哪种情况更合理？

步骤如下：

1. 将这个句子进行中文分词，同时得到几种不同的分词结果，产生歧义。
2. 拿到这篇文章，然后求当前要分词的句子的出现概率。通过链式法则和三元文法模型，可以计算这种分词结果在整个文档集合中出现的概率。
3. 将几种分词结果计算出来的词在文章中出现的概率进行排序，即可得知最精准的那一种。

越多背景信息，越强的相关性。

## 精选评论

Q：针对语料不是很充足的文本集，如何使用全篇文章的语义（不只是前面词的上下文关系）来设计一个文本分类器？
A：一种做法是增大 ngram 里面的 n，因为标注数据不多，增加 n 不会增加太多的存储空间。另外，也可以使用少量数据训练得到的分类器，对新的数据进行分类，然后获得一些分类结果后，人工再进行复查，把正确的结果再次纳入训练数据。
