# 27 | 决策树：信息增益、增益比率和基尼指数的运用

## 如何实现更简短的问卷？

我们先选择信息增益最高的问题，这样熵值就下降的最快。

以之前武侠 case 为例，有些 2 个问题就可以分类，最多只需要 3 个即可全部分类。如果先选择熵值最低的问题，最差的情况下，需要问所有五个问题，而且大部分在 4 个问题左右。

所以我们可以通过熵值的自动编排实现优化问卷设计的目的。训练方式如下：

1. 根据集合中的样本分类，为每个集合计算信息熵，并通过全部集合的熵值加权平均，获得整个数据集的熵。一开始集合只有一个，并且包含了所有的样本。
2. 根据信息增益，计算每个特征的区分能力。挑选区分能力最强的特征，并对每个集合进行更细的划分。
3. 有了新的划分之后，回到第一步并进行重复，直到没有更多特征。

这也是训练决策树的基本思想。实际情况下，由于每个类型对应多个样本，所以决策树通常只能把整体的熵降低到一个比较低的值，而无法完全降到 0。

## 几种决策树算法

### 采用信息增益构建决策树的 ID3 和 C4.5 算法

ID3 有个问题是会优先考虑具有较多取值的特征，比如情绪特征，有几十种值，那么基于这样一个特征分出来的很多很多组相对会有比较大的信息增益。

因此容易导致机器学习中的过拟合现象，从而不利于决策树对新数据的预测。

> 过拟合：举个例子，你学习了背诵了所有试卷的试题，精准度非常高，只要出现了这道题肯定就可以匹配上做出来。但即便是调整了一个小参数，调整了一下题目，就完蛋了。所以对于老数据非常精准，但是对于新数据的预测就不行了。

C4.5 使用信息增益率来代替信息增益，降低决策树过拟合的成都。它通过引入一个被称作分裂信息的项来惩罚取值较多的特征。分裂信息会考虑子集的数量，如果特征取值很多，就会产生很多子集的数量，产生的分裂信息的值就会变大。

因此通过 信息增益 / 分裂信息 得到的 信息增益率，就可以减少特征值过多带来的过拟合效应。

### 采用基尼指数（Gini）的 CART 算法（分类与回归树）

CART 不再使用信息增益或者信息增益率，而是采用基尼指数来选择最好的特征。CARt 算法采用了二叉树，每次把数据切成两份，分别进入左子树、右子树。

CART 每一次迭代都会降低基尼指数，类似 ID3、C4.5 降低信息熵的过程。
