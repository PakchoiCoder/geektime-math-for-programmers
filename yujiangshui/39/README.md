# 39 | 线性回归（上）：如何使用高斯消元求解线性方程组？

回归分析属于监督式学习算法，主要研究一个或者多个随机变量 y 与另外一些变量 x 之间的关系，其中 y 成为因变量，x 称为自变量。可以分为三种：

- 按照自变量数量，当自变量 x 的个数大于 1 时，就是多元回归
- 按照因变量数量，当因变量 y 的个数大于 1 时，就是多重回归
- 按照模型种类，如果因变量和自变量为线性关系时，就是线性回归模型；如果因变量和自变量为非线性关系时，就是非线性回归分析模型

## 高斯消元法

一元线性回归模型：Y = a + bX + ε，X 是自变量，Y 是因变量，a 是截距，b 是自变量的系数，ε 是随机误差。如果我们忽略 a 和 ε 然后扩展成多元的形式就变成：

> b1*x1 + b2*x2 ... = y

假设有多个这样的方程，就可以构成线性方程组。求解主要有：直接法和迭代法。

直接法就是通过有限次的算术运算，计算精确解。而迭代法，我们在第 3 讲就提到过，它是一种不断用变量的旧值递推新值的过程。我们可以用迭代法不断地逼近方程的精确解。

高斯消元法主要分为两步，消元（Forward Elimination）和回代（Back Substitution）。消元，就是减少某些方程中元的数量，如果方程只剩下一个 x 那么就知道它的解了。回代就是把已知解代入方程，求其他。

### 消元

我们保持第一个方程不动，逐步用其他方程对该方程运算，消掉 x1，这里第一个方程就称为”主元行“。

然后挨个方程消掉，之后再把第二个方程作为“主元行”消掉第三个方程的 x2，如果第三个方程没有，就调换一下。

之后就可以发现，第一个方程有三个自变量，第二个有两个，第三个有一个，就可以开始回代，从第三个开始倒着来，得到所有解。

### 使用矩阵实现高斯消元法

数量少的比较简单，数量多了，就可以用矩阵来实现，方便理解和记忆。

首先先把系数转换成矩阵。

之后就是对系数进行消元，过程就是把原始的系数矩阵，通过一定的乘积运算等，变成上三角矩阵（左边是 0），矩阵中只有主对角线以及主对角线以上的三角部分有数字。

回代的过程就是把上三角矩阵变成单位矩阵的过程。单位矩阵就是斜线有值（表示一个个的变量），其余为 0。

实际运算中，为了方便回代计算，需要把等号右边的值加入系数矩阵，这个新矩阵称为增广矩阵。

对于这个增广矩阵，我们最终目标就是把除了最后一列之外的部分，变成单位矩阵，之后，每一列最后的值就是这个自变量对应的解了。

最后可以发现利用消元法进行线性方程组求解的过程，就是在找系数矩阵的逆矩阵的过程。

## 总结

从矩阵的角度来说，消元就是把系数矩阵变为上三角矩阵，而回代就是把这个上三角矩阵变为单位矩阵。

在进行线性回归分析时，方程组的处理方式和普通的方程组求解有一些不同，两个主要区别：

1. 线性回归分析中，样本数据会告诉我们自变量和因变量的值，求系数。线性方程组中，一直系数和因变量的值，求自变量的值。
2. 线性回归分析，方程的数量远大于自变量的数量，而且不要求每个方程式都是成立的。拟合出来的因变量值可以和样本数据给定的因变量值存在差异，允许模型你和存在误差。求解线性方程组就需要精确解。

因此无法利用消元法来求解线性回归。
