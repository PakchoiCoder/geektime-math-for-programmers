# 28 | 熵、信息增益和卡方：如何寻找关键特征？

机器学习的步骤主要包括数据的准备、特征工程、模型拟合、离线和在线测试。测试过程也许会产生新的数据，用于进一步提升模型。在这些处理中，特征工程是非常重要的一步。

越来越多的数据类型和纬度的出现，会加大机器学习的难度，并影响最终的准确度。所以需要用特征选择来过滤不必要的噪音特征。因此诞生了一类基于分类标签的选择方法，通过信息论的一些统计度量，看特征和类标签的关联程度有多大。

## 利用信息熵进行特征选择

一个好的特征选择，应该可以把那些对分类有价值的信息提取出来，而过滤掉那些对分类没有什么价值的信息。可以利用分类标签来进行挑选，通过信息熵和信息增益来计算获取有价值的特征。

对分类有价值的特征：

经常只在某个或者少数几个分类中出现，很少在其他分类中出现，说明这个特征有较强的区分能力，它的出现很可能预示着数据属于某个分类的概率很高。

是否属于少数几个类这一点，可以使用信息熵来衡量。可以使用如下的公式计算：

![](https://static001.geekbang.org/resource/image/4f/6e/4f316dd9824e5522a71a8de54102796e.png)

如果熵值低，说明包含这个特征的数据只出现在少数分类中。但是这个做法只考虑了单个特征出现时对应数据的分类情况，并没有考虑整个数据集的分类情况。

可以借助决策树的信息增益概念继续计算进行比较和排序，公式如下：

![](https://static001.geekbang.org/resource/image/a6/34/a6a9e6ce3eab1fa755488f8c82c1ac34.png)

如果信息增益越大，就说明这个特征对分类的判断越有价值。

## 利用卡方检验进行特征选择

在统计学中，我们使用卡方检验来检验两个变量是否相互独立。把它运用到特征选择，我们就可以检验特征与分类这两个变量是否独立。如果两者独立，证明特征和分类没有明显的相关性，特征对于分类来说没有提供足够的信息量。

## 总结

无论是使用何种统计度量，我们都可以计算相应的数值、排序、并得到排名靠前的若干特征。从文本分类的角度来说，我们只会挑选对分类最有价值的那些单词或词组，而去除其他不重要的那些词。如果特征选择得当，我们既可以减少模型存储的空间，还可以提升分类的准确度。当然，过度的减少特征最终会导致准确度的下降，所以对于不同的数据集要结合实验，要把握一个合理的度。
