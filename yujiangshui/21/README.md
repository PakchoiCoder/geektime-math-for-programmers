# 21 | 概率基础（下）：联合概率、条件概率和贝叶斯法则，这些概率公式究竟能做什么？

想要更加精确的描述这些现象，就需要理解几个新的概念，联合概率、条件概率以及贝叶斯法则。

## 联合概率、条件概率和边缘概率

由多个随机变量决定的概率叫做联合概率（用 P(x, y) 表示），它的概率分布就是联合概率分布。

对于离散型随机变量，我们可以通过通过联合概率 P(x, y) 在 y 上求和，就可以得到 P(x)。对于连续型随机变量，我们可以通过联合概率 P(x, y) 在 y 上的积分，推导出概率 P(x)。这个时候，我们称 P(x) 为边缘概率。

条件概率也是由多个随机变量决定，但是和联合概率不同的是，它计算了给定某个（或多个）随机变量的情况下，另一个（或多个）随机变量出现的概率，其概率分布叫做条件概率分布。给定随机变量 x，随机变量 y 的条件概率使用 P(y | x) 表示。比如：限定在男生的条件下，考取 80 分以上的有多少人？原本男生和女生也是一个随机变量。

联合概率是条件概率和概率的乘积。比如条件概率：男生大于 90 分的，男生的概率是 50%，那么联合概率就是 条件概率 \* 男生的概率。

## 贝叶斯定理

![](https://static001.geekbang.org/resource/image/d0/79/d082a9fc31d9378f4511dc23807df179.png)

在这个公式中，还包含了先验概率（Prior Probability）、似然函数（Likelihood）、边缘概率（Marginal Probability）和后验概率（Posterior Probability）的概念。P(x) 称为先验概率。之所以称为“先验”，是因为它是从数据资料统计得到的，不需要经过贝叶斯定理的推算。

贝叶斯定理定义了先验概率、后验概率和似然函数，后验概率和似然函数及先验概率的乘积成正比关系。如果有一定数量的标注数据，那么通过统计的方法，我们可以很方便地得到先验概率和似然函数，然后推算出后验概率，最后依据后验概率来做预测。这整个过程符合监督式机器学习的模型训练和新数据预测这两个阶段，因此朴素贝叶斯算法被广泛运用在机器学习的分类问题中。

## 随机变量之间的独立性

如果随机变量之间是相互独立的，带入贝叶斯公式之后，就可以得出 P(x, y) = P(x)xP(y) 从而简化概率计算。

例如将联合概率 p(x1​, x2​, x3​, x4​, x5​, x6​) 转换为 p(x1​) _ p(x2​) _ p(x3​) _ p(x4​) _ p(x5​) \* p(x6​)。

在实际项目中，我们会假设多个随机变量是相互独立的，大幅简化计算，降低对数据统计量的要求，虽然通常不成立，但是可以得到近似的解。相比精准度来说，可行性更为重要。

## 思考题

已知爷爷有 4 个 50，6 个 100，奶奶有 8 个 50，4 个 100，所以得知 P(100) P(50) P(爷爷) P(奶奶），然后求 P(100|爷爷) 。

P(100|爷爷) = P(爷爷|100) \* P(100) / P(爷爷)，

P(100) = 10 / 22 = 45.4545455
P(爷爷) = 10 / 22 = 45.4545455
P(爷爷|100) = 6 / 10 = 60

P(100|爷爷) = 60 \* 45 / 45 = 60%
