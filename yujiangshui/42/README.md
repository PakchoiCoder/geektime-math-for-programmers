# 42 | PCA 主成分分析（上）：如何利用协方差矩阵来降维？

两种更为通用的降维方法：

- 主成分分析 PCA（Principal Component Analysis）
- 奇异值分解 SVD（Singular Value Decomposition）

这两种方法是从矩阵分析的角度出发，找到数据分布之间的关系，从而达到降低维度的目的，因此不需要监督学习中样本标签和特征之间的关系。

## PCA 主要步骤

### 什么是特征的降维

在机器学习中，我们把物品的特征转换成计算机所能处理的各种数据。增加物品的特征，有可能增加效果，但是特征太多，维度就会升高，增加机器学习的难度和准确度。所以我们需要过滤掉一些不重要的特征或者合并起来。

已知一个样本矩阵，每一行表示一个样本，每一列表示一维特征。能不能通过某种方法，找到一种变换，可以降低这矩阵的烈属，尽可能保留原始数据中有用的信息？

### 步骤

1. 标准化样本矩阵中的原始数据
2. 获取标准化数据的协方差矩阵
3. 计算协方差矩阵的特征值和特征向量
4. 依照特征值的大小，挑选主要的特征向量
5. 生成新的特征

#### 标准化原始数据

通过一个标准化公式，让每维特征的重要性具有可比性。之后才可以把重要性低的去掉。

#### 获取协方差（Covariance）矩阵

协方差用于衡量两个变量的总体误差。

![](https://static001.geekbang.org/resource/image/27/c2/2732d3255408c3bb4e01f6c2bd4499c2.png)

其中 xk​ 表示变量 x 的第 k 个采样数据，xˉ 表示这 k 个采样的平均值。而当两个变量是相同时，协方差就变成了方差。

将矩阵中每个值做协方差计算，就可以得到一个对称矩阵。这个对称矩阵的主对角线上的值就是各维特征的方差。

#### 计算协方差矩阵的特征值和特征向量

这里的矩阵的特征向量和机器学习中的特征向量是两回事。对于一个矩阵 X 如果能找到向量 v 和标量 λ，使得下面这个式子成立。Xv = λv 那么，我们就说 v 是矩阵 X 的特征向量，而 λ 是矩阵 X 的特征值。

todo

之后可以获取特征向量。

#### 挑选主要特征向量，转换原始数据

假设获得了 k 各特征值和对应的特征向量，那么按照对应的 λ 进行排序获得 v 的排序，排名靠前的就是最重要的特征向量。

假设获取前 k1 各最重要的特征，就可以组成一个 nxk1 维的矩阵 D。然后将原始矩阵左乘矩阵 D，就可以获得一个新的矩阵，达到降维的目的。

有的时候，我们无法确定 k1 取多少合适。一种常见的做法是，看前 k1 个特征值的和占所有特征值总和的百分比。假设一共有 10 个特征值，总和是 100，最大的特征值是 80，那么第一大特征值占整个特征值之和的 80%，我们认为它能表示 80% 的信息量，还不够多。那我们就继续看第二大的特征值，它是 15，前两个特征值之和有 95，占比达到了 95%，如果我们认为足够了，那么就可以只选前两大特征值，把原始数据的特征维度从 10 维降到 2 维。
